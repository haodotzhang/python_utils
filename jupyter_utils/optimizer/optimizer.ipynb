{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.配置文件\n",
    "# 未来读取这个dict，并返回value部分. 具体见 conf/algo_conf.json\n",
    "# self.optimizer_conf = self.conf[\"algo_conf\"][\"optimizer\"]\n",
    "\"optimizer\":{\n",
    "    \"FTRL\": {\n",
    "        \"scope\": \"Wide\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"learning_rate_power\": -0.5,\n",
    "        \"initial_accumulator_value\": 0.1,\n",
    "        \"l1_regularization_strength\": 2.0,\n",
    "        \"l2_regularization_strength\": 7.0,\n",
    "        \"use_locking\": False,\n",
    "    },\n",
    "    \"AdamAsync\": {\n",
    "        \"scope\": \"Global\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"use_locking\": False,\n",
    "        \"clip_gradients\": 5.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import optimize_loss\n",
    "\n",
    "# 2.调用部分\n",
    "class MyEstimator(object):\n",
    "    def build_graph(self, ):\n",
    "        logits, labels, weights = -1, -1, -1 # 通过一些辅助方法获取\n",
    "        loss = self.compute_loss(logits, labels, weights)\n",
    "        #\n",
    "        train_op = self.optimize(loss)\n",
    "\n",
    "    # 3.optimizer()部分\n",
    "    def optimize(self, loss):\n",
    "        with tf.name_scope(\"Optimize\"):\n",
    "            # update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            train_ops = []\n",
    "            # 两类variables: 一类是配置文件中通过scope特殊指定的，另一类就是其他(记作global或者default scope)\n",
    "            scope_variables, global_variables = set(), []\n",
    "            global_optimizer = None\n",
    "\n",
    "            for name, config in self.optimizer_conf.items():\n",
    "                # 读取配置文件\n",
    "                optimizer = self.find_optimizer(name, config) # for 某个tower, name即为 tower_scope_name\n",
    "                scope = config['scope'] # for 某个totwer, 后面tower的scope name也要与这个配置保持一致\n",
    "\n",
    "                # global_variables的optimizer_ops的设置分拆了两部分，当前仅记录必要信息\n",
    "                if scope.lower() == 'global':\n",
    "                    global_optimizer = (optimizer, config) # for 默认tower\n",
    "                    continue # global_optimizer的后续配置在循环外\n",
    "                \n",
    "                # 1) 设置 scope_variables 的optimizer_ops\n",
    "                with tf.name_scope(scope):\n",
    "                    self.logger.info('Set optimizer for scope: %s' % scope)\n",
    "                    #\n",
    "                    # 1.1) 获取scope_variables (非global scope的其他scope)\n",
    "                    variables = tf.trainable_variables(scope)\n",
    "                    if not variables:\n",
    "                        self.logger.warn(\"No variables to optimize in scope: %s\" % scope)\n",
    "                        continue # 当前scope(即tower)无待训练参数，可以直接跳过\n",
    "                    scope_variables.update(variables)\n",
    "                    #\n",
    "                    # 1.2) 设置scope_variables的optimizer_ops\n",
    "                    cur_opt_scope = self.optimize_scope(loss, variables, optimizer, \n",
    "                                                        config.get('clip_gradients', None))\n",
    "                    train_ops.append(cur_opt_scope)\n",
    "\n",
    "            # 2) 设置 global_variables 的optimizer_ops\n",
    "            # scope_variables之外的其他variables, 都视做一个default scope，共享一个默认的optimizer\n",
    "            with tf.name_scope('Global'):\n",
    "                self.logger.info('Set optimizer for scope: global scope')\n",
    "                # 2.1) 获取global_variables (准确说是 other_variables)\n",
    "                global_variables = [\n",
    "                    var for var in tf.trainable_variables()\n",
    "                    if var not in scope_variables\n",
    "                ]\n",
    "\n",
    "                if global_variables:\n",
    "                    if not global_optimizer:\n",
    "                        raise ValueError(\"Config Error: no global_optimizer for variables.\") # default global 必须配置\n",
    "\n",
    "                # 2.2 ）设置 global_variables 的optimizer_ops\n",
    "                optimizer, config = global_optimizer\n",
    "                cur_opt_scope = self.optimize_scope(loss, global_variables, optimizer, \n",
    "                                                    config.get('clip_gradients', None))\n",
    "                train_ops.append(cur_opt_scope)\n",
    "\n",
    "            if len(train_ops) > 1:\n",
    "                train_op = tf.group(*train_ops)\n",
    "            elif len(train_ops) == 1:\n",
    "                train_op = train_ops[0]\n",
    "            else:\n",
    "                raise ValueError(\"Config Error: insufficinet scope.\")\n",
    "\n",
    "            return train_op\n",
    "\n",
    "    def find_optimizer(opt_name, config_kv):\n",
    "        lr = config_kv.get(\"learning_rate\", 0.0001)\n",
    "        use_locking = config_kv.get(\"use_locking\", False)\n",
    "\n",
    "        # 遍历匹配\n",
    "        if opt_name == \"Adagrad\":\n",
    "            opt = tf.train.AdagradOptimizer(\n",
    "                learning_rate=lr,\n",
    "                initial_accumulator_value=config_kv.get(\n",
    "                    \"initial_accumulator_value\", 0.1\n",
    "                ),\n",
    "                use_locking=use_locking\n",
    "            )\n",
    "        elif opt_name == \"Adam\":\n",
    "            opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=lr,\n",
    "                beta1=config_kv.get(\"beta1\", 0.9),\n",
    "                beta2=config_kv.get(\"beta2\", 0.999),\n",
    "                epsilon=config_kv.get(\"epsilon\", 1e-8),\n",
    "                use_locking=use_locking\n",
    "            )\n",
    "        elif opt_name == \"AdamAsync\":\n",
    "            opt = tf.train.AdamAsyncOptimizer(\n",
    "                learning_rate=lr,\n",
    "                beta1=config_kv.get(\"beta1\", 0.9),\n",
    "                beta2=config_kv.get(\"beta2\", 0.999),\n",
    "                epsilon=config_kv.get(\"epsilon\", 1e-8),\n",
    "                use_locking=use_locking\n",
    "            )\n",
    "        elif opt_name == \"Adadelta\":\n",
    "            opt = tf.train.AdadeltaOptimizer(\n",
    "                learning_rate=lr,\n",
    "                rho=config_kv.get(\"rho\", 0.95),\n",
    "                epsilon=config_kv.get(\"epsilon\", 1e-8),\n",
    "                use_locking=use_locking\n",
    "            )\n",
    "        elif opt_name == \"Momentum\":\n",
    "            opt = tf.train.MomentumOptimizer(\n",
    "                learning_rate=lr,\n",
    "                momentum=config_kv.get(\"momentum\", 0.0),\n",
    "                use_nesterov=config_kv.get(\"use_nesterov\", False),\n",
    "                use_locking=use_locking\n",
    "            )\n",
    "        elif opt_name == \"Ftrl\":\n",
    "            opt = tf.train.FtrlOptimizer(\n",
    "                learning_rate=lr,\n",
    "                learning_rate_power=config_kv.get(\"learning_rate_power\", 0.05),\n",
    "                initial_accumulator_value=config_kv.get(\"initial_accumulator_value\", 0.1),\n",
    "                l1_regularization_strength=config_kv.get(\"l1_regularization_strength\", 0.0),\n",
    "                l2_regularization_strength=config_kv.get(\"l2_regularization_strength\", 0.0),\n",
    "                l2_shrinkage_regularization_strength=config_kv.get(\n",
    "                    \"l2_shrinkage_regularization_strength\", 0.0\n",
    "                ),\n",
    "                use_locking=use_locking\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer Error.\")\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    def optimize_scope(self, loss, variables, optimizer, clip_gradients=None):\n",
    "        # 全部待bp的ops\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            return optimize_loss(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                learning_rate=None, \n",
    "                optimizer=optimizer, \n",
    "                clip_gradients=clip_gradients,\n",
    "                update_ops=update_ops,\n",
    "                variables=variables,\n",
    "                summaries=[\n",
    "                    \"learning_rate\",\n",
    "                    \"loss\",\n",
    "                    \"global_gradient_norm\",\n",
    "                ],\n",
    "                increment_global_step=False,\n",
    "            )\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
